{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6fcd72a-e2f7-47b2-8874-4778721748e6",
   "metadata": {},
   "source": [
    "# DSX Dev GenAI Text to Text Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48f33f57",
   "metadata": {},
   "source": [
    "Following code demonstrates how to use text generation models using Dev GenAI. This notebook serves as an example. For more documentation regarding the usage of Dev GenAI and other products please refer to our [Dev GenAI Confluence page](https://confluence.dell.com/display/DSX/Request+GenAI+Model+Access)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7d92197",
   "metadata": {},
   "source": [
    "#### Installing Packages\n",
    "\n",
    "Run the following cell to install required packages to run this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a1b8b-d191-4f26-8587-e31034a27fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.6' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '\"c:/Program Files/Python312/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install 'openai>=1.45.0' requests python-dotenv certifi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14857e82-b701-4148-a6fe-8b2c6cc84e20",
   "metadata": {},
   "source": [
    "### Intial setup\n",
    "### Setup API key\n",
    "\n",
    "Environment variables are loaded using dotenv, ensuring sensitive information like API keys are not hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9ccbc-79fe-4658-9697-21e323e74304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# At first, add your api key to .env file as DEV_GENAI_API_KEY='Insert_your_Dev_GenAI_Text_to_Text_API_key_here' and load env file\n",
    "load_dotenv('.env', override=True)\n",
    "\n",
    "# alternatively you can export an enviroment variable using following command: export DEV_GENAI_API_KEY=\"Insert_your_Dev_GenAI_Text_to_Text_API_key_here\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5836db1",
   "metadata": {},
   "source": [
    "### Update certifi bundle with Dell certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7da61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import certifi\n",
    "\n",
    "def update_certifi():\n",
    "    # URL to download the Dell certificates zip file\n",
    "    url = \"https://pki.dell.com//Dell%20Technologies%20PKI%202018%20B64_PEM.zip\"\n",
    "    print(\"Downloading Dell certificates zip from:\", url)\n",
    "    response = requests.get(url)\n",
    "    # Use raise_for_status() for concise error checking\n",
    "    response.raise_for_status()\n",
    "    print(\"Downloaded certificate zip, size:\", len(response.content), \"bytes\")\n",
    "\n",
    "    # Determine the location of the certifi bundle\n",
    "    cert_path = certifi.where()\n",
    "    print(\"Certifi bundle path:\", cert_path)\n",
    "\n",
    "    # Define the names of the certificates within the zip file\n",
    "    dell_root_cert_name = \"Dell Technologies Root Certificate Authority 2018.pem\"\n",
    "    dell_issuing_cert_name = \"Dell Technologies Issuing CA 101_new.pem\"\n",
    "\n",
    "    # Append the certificates directly from the zip archive in memory.\n",
    "    print(\"Appending Dell certificates to certifi bundle...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Read certificate contents directly from the zip file in memory\n",
    "            # Ensure decoding from bytes to string (assuming UTF-8)\n",
    "            root_cert_content = z.read(dell_root_cert_name).decode('utf-8')\n",
    "            issuing_cert_content = z.read(dell_issuing_cert_name).decode('utf-8')\n",
    "\n",
    "            # Append the certificates to the certifi bundle\n",
    "            # (Make sure you have backup of certifi bundle if needed.)\n",
    "            with open(cert_path, \"a\") as bundle:\n",
    "                bundle.write(\"\\n\")\n",
    "                bundle.write(root_cert_content)\n",
    "                bundle.write(\"\\n\") # Ensure newline after first cert\n",
    "                bundle.write(issuing_cert_content)\n",
    "                bundle.write(\"\\n\") # Ensure newline after second cert\n",
    "\n",
    "        print(\"Dell certificates successfully added to certifi bundle.\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        # Handle case where expected certificate file is not in the zip\n",
    "        print(f\"Error: Certificate file '{e}' not found in the zip archive.\")\n",
    "    except Exception as e:\n",
    "        # Handle other potential errors during processing\n",
    "        print(f\"An error occurred during certificate appending: {e}\")\n",
    "\n",
    "\n",
    "update_certifi()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a9258db-c509-464c-8bc6-eacceda3974a",
   "metadata": {},
   "source": [
    "### OpenAI client initialization\n",
    "This section of code initializes the Azure Open AI client, which is necessary to interact with OpenAI's API using Python. \n",
    "\n",
    "#### Useful Links:\n",
    "\n",
    "* OpenAI API Documentation: [OpenAI API](https://platform.openai.com/docs/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08175a53-2ef2-46eb-b766-6b11e3730d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import httpx\n",
    "import os\n",
    "\n",
    "http_client=httpx.Client(verify=certifi.where())\n",
    "client = OpenAI(\n",
    "    base_url='https://genai-api-dev.dell.com/v1',\n",
    "    http_client=http_client,\n",
    "    api_key=os.environ[\"DEV_GENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8ad7c6d-62f0-451f-9d28-fdb361aecfd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dev GenAI responses leveraging OpenAI package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d30e7128-8f5f-433d-95ca-4e02fe84f1a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get results from a simple prompt (OpenAI Completion)\n",
    "\n",
    "This example demonstrates how to use OpenAI client to get responses from `mixtral-8x7b-instruct-v01`, `llamaguard-7b`, `mistral-7b-instruct-v03`, `phi-3-mini-128k-instruct`, `phi-3-5-moe-instruct`, `llama-3-8b-instruct`, `llama-3-1-8b-instruct`, `llama-3-2-3b-instruct`, `codellama-13b-instruct`, `sqlcoder-7b-2`, `codestral-22b-v0-1`,`llama-3-3-70b-instruct` models. The `completions.create` method is used, where a prompt is provided, and the model generates a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da986f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = True\n",
    "max_output_tokens = 200\n",
    "\n",
    "# Available Models list\n",
    "available_models = [\"mixtral-8x7b-instruct-v01\", \"llamaguard-7b\", \"mistral-7b-instruct-v03\", \"phi-3-mini-128k-instruct\", \"phi-3-5-moe-instruct\", \"llama-3-8b-instruct\", \"llama-3-1-8b-instruct\", \"llama-3-2-3b-instruct\",\"codellama-13b-instruct\", \"sqlcoder-7b-2\", \"codestral-22b-v0-1\", \"llama-3-3-70b-instruct\"]\n",
    "\n",
    "# Let's select the model from available list\n",
    "model_selected = available_models[0]\n",
    "\n",
    "for model_selected in available_models:\n",
    "    print(f\"Model: {model_selected}\")\n",
    "    completion = client.completions.create(\n",
    "        model=model_selected,\n",
    "        max_tokens=max_output_tokens,\n",
    "        prompt=f'Can you explain who are the Los Angeles Dodgers and what are they known for is in less than {max_output_tokens} tokens?',\n",
    "        stream=streaming)\n",
    "\n",
    "    if streaming:\n",
    "        for chunk in completion:\n",
    "            print(chunk.choices[0].text, end='')\n",
    "    else:\n",
    "        print(completion.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6030d76f",
   "metadata": {},
   "source": [
    "\n",
    "### Get results from OpenAI Chat Completion\n",
    "\n",
    "This example demonstrates how to use the Dev GenAI client to get responses from a chat model `mixtral-8x7b-instruct-v01`, `llamaguard-7b`, `mistral-7b-instruct-v03`, `phi-3-mini-128k-instruct`, `phi-3-5-moe-instruct`, `llama-3-8b-instruct`, `llama-3-1-8b-instruct`, `llama-3-2-3b-instruct`, `codellama-13b-instruct`, `codestral-22b-v0-1`, `llama-3-3-70b-instruct`. The `chat.completions.create` method is used, where a conversation context is provided, and the model generates a continuation. In this case, the conversation is about the favourite condiment. This is useful for building conversational AI applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bfeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "streaming = False # To enable streaming, set streaming to True\n",
    "available_models = [\"mixtral-8x7b-instruct-v01\", \"llamaguard-7b\", \"mistral-7b-instruct-v03\", \"phi-3-mini-128k-instruct\", \"phi-3-5-moe-instruct\", \"llama-3-8b-instruct\", \"llama-3-1-8b-instruct\", \"llama-3-2-3b-instruct\",\"codellama-13b-instruct\", \"codestral-22b-v0-1\", \"llama-3-3-70b-instruct\"]\n",
    "selected_model = available_models[5]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=selected_model,\n",
    "    messages = [\n",
    "            {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "            {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "        ],\n",
    "    stream=streaming\n",
    ")\n",
    "\n",
    "if streaming:\n",
    "    for chunk in completion:\n",
    "        if chunk.id:\n",
    "            if chunk.choices[0].delta.content == None and chunk.choices[0].delta.role != None:\n",
    "                print(chunk.choices[0].delta.role+': ', end='')\n",
    "            elif chunk.choices[0].delta.content != None:\n",
    "                print(chunk.choices[0].delta.content, end='')\n",
    "else:\n",
    "    print(completion.choices[0].message.role + ': ' + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df7108",
   "metadata": {},
   "source": [
    "### Get results for a code generator model (OpenAI Completion)\n",
    "\n",
    "This example demonstrates how to use OpenAI client to get responses from `codellama-13b-instruct`, `sqlcoder-7b-2`, `codestral-22b-v0-1`, `llama-3-sqlcoder-8b` models. The `completions.create` method is used, where a prompt is provided, and the model generates a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6813d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = True\n",
    "max_output_tokens = 200\n",
    "\n",
    "sqlcoder_prompt = \"\"\"### Task\n",
    "                        Generate a SQL query to answer [QUESTION] join the two tables and count for unique ids[/QUESTION]\n",
    "                        ### Database Schema\n",
    "                        The query will run on a database with the following schema: test_schema.table1 and test_schema.table2\n",
    "                        ### Answer\n",
    "                        Given the database schema, here is the SQL query that [QUESTION] join the two tables and count for unique ids[/QUESTION]\n",
    "                        [SQL]\n",
    "                \"\"\"\n",
    "codellama_prompt = \"Write a python function to generate the nth fibonacci number.\"\n",
    "\n",
    "# prompt pieces for llama-3-sqlcoder-8b\n",
    "user_question = \"What are our top 3 products by revenue in the New York region?\"\n",
    "instructions = \"- if the question cannot be answered given the database schema, return \\\"I do not know\\\"\\n- recall that the current date in YYYY-MM-DD format is 2024-09-13\"\n",
    "create_table_statements = \"\"\"\n",
    "CREATE TABLE products (\n",
    "  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n",
    "  name VARCHAR(50), -- Name of the product\n",
    "  price DECIMAL(10,2), -- Price of each unit of the product\n",
    "  quantity INTEGER  -- Current quantity in stock\n",
    ");\n",
    "\n",
    "CREATE TABLE customers (\n",
    "  customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n",
    "  name VARCHAR(50), -- Name of the customer\n",
    "  address VARCHAR(100) -- Mailing address of the customer\n",
    ");\n",
    "\n",
    "CREATE TABLE salespeople (\n",
    "  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson \n",
    "  name VARCHAR(50), -- Name of the salesperson\n",
    "  region VARCHAR(50) -- Geographic sales region \n",
    ");\n",
    "\n",
    "CREATE TABLE sales (\n",
    "  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n",
    "  product_id INTEGER, -- ID of product sold\n",
    "  customer_id INTEGER,  -- ID of customer who made purchase\n",
    "  salesperson_id INTEGER, -- ID of salesperson who made the sale\n",
    "  sale_date DATE, -- Date the sale occurred \n",
    "  quantity INTEGER -- Quantity of product sold\n",
    ");\n",
    "\n",
    "CREATE TABLE product_suppliers (\n",
    "  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n",
    "  product_id INTEGER, -- Product ID supplied\n",
    "  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n",
    ");\n",
    "\n",
    "-- sales.product_id can be joined with products.product_id\n",
    "-- sales.customer_id can be joined with customers.customer_id \n",
    "-- sales.salesperson_id can be joined with salespeople.salesperson_id\n",
    "-- product_suppliers.product_id can be joined with products.product_id\n",
    "\"\"\"\n",
    "sqlcoder_8b_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "                           Generate a SQL query to answer this question: `{user_question}`\n",
    "                           {instructions}\n",
    "                           DDL statements:\n",
    "                           {create_table_statements}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "                           The following SQL query best answers the question `{user_question}`:\n",
    "                           ```sql\n",
    "                           \"\"\"\n",
    "\n",
    "\n",
    "available_models = [\"codellama-13b-instruct\", \"sqlcoder-7b-2\", \"codestral-22b-v0-1\", \"llama-3-sqlcoder-8b\"]\n",
    "# Let's select the model from available list\n",
    "model_selected = available_models[0]\n",
    "\n",
    "\n",
    "# Available Prompts per model list\n",
    "models_prompts = {\"codellama-13b-instruct\" : codellama_prompt, \n",
    "                  \"sqlcoder-7b-2\": sqlcoder_prompt,\n",
    "                  \"llama-3-sqlcoder-8b\": sqlcoder_8b_prompt,\n",
    "                  \"codestral-22b-v0-1\": codellama_prompt\n",
    "                 }\n",
    "\n",
    "# Let's select the prompt from available list\n",
    "prompt_selected = models_prompts[model_selected]\n",
    "\n",
    "\n",
    "print(f\"Model: {model_selected}\")\n",
    "completion = client.completions.create(\n",
    "    model=model_selected,\n",
    "    \n",
    "    max_tokens=max_output_tokens,\n",
    "    prompt=prompt_selected,\n",
    "    stream=streaming)\n",
    "\n",
    "if streaming:\n",
    "    for chunk in completion:\n",
    "        print(chunk.choices[0].text, end='')\n",
    "else:\n",
    "    print(completion.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb501166",
   "metadata": {},
   "source": [
    "## II. Get Dev GenAI responses leveraging Python requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e1f0b44",
   "metadata": {},
   "source": [
    "### II a. Python request Completion\n",
    "\n",
    "This example demonstrates how to get responses from `mixtral-8x7b-instruct-v01`, `llamaguard-7b`, `mistral-7b-instruct-v03`, `phi-3-mini-128k-instruct`, `phi-3-5-moe-instruct`, `llama-3-8b-instruct`, `llama-3-1-8b-instruct`, `llama-3-2-3b-instruct`, `llama-3-3-70b-instruct` models using python request. We will be sending headers including API-Key and json data that includes prompt to `v1/completions` endpoint, and the model generates a response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbadef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "streaming = True\n",
    "max_output_tokens = 200\n",
    "\n",
    "# Available Models list\n",
    "available_models = [\"mixtral-8x7b-instruct-v01\", \"llamaguard-7b\", \"mistral-7b-instruct-v03\", \"phi-3-mini-128k-instruct\", \"phi-3-5-moe-instruct\", \"llama-3-8b-instruct\", \"llama-3-1-8b-instruct\", \"llama-3-2-3b-instruct\", \"llama-3-3-70b-instruct\"]\n",
    "\n",
    "# Let's select the model from available list\n",
    "model_selected = available_models[0]\n",
    "\n",
    "def stream_and_yield_response(response):\n",
    "    for chunk in response.iter_lines():\n",
    "        decoded_chunk = chunk.decode(\"utf-8\")\n",
    "        if decoded_chunk == \"data: [DONE]\":\n",
    "            pass\n",
    "        elif decoded_chunk.startswith(\"data: {\"):\n",
    "            payload = decoded_chunk.lstrip(\"data:\")\n",
    "            json_payload = json.loads(payload)\n",
    "            yield json_payload['choices'][0]['text']\n",
    "\n",
    "\n",
    "# function from confluence\n",
    "def llm_api(data):\n",
    "    \"\"\"\n",
    "    Create a request to Dev GenAI Text to Text model with API key in header.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = f\"https://genai-api-dev.dell.com/v1/completions\"\n",
    "      \n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'api-key': os.environ[\"DEV_GENAI_API_KEY\"],\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "      \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, stream=data['stream'], verify=certifi.where())\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if data['stream']:\n",
    "            for result in stream_and_yield_response(response):\n",
    "                print(result, end='')\n",
    "        else:\n",
    "            response_dict = response.json()\n",
    "            result = response_dict['choices'][0]['text']\n",
    "            print(result)\n",
    "           \n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print('Error code:', err.response.status_code)\n",
    "        print('Error message:', err.response.text)\n",
    "    except Exception as err:\n",
    "        print('Error:', err)\n",
    "\n",
    "# Model instruction and Parameters\n",
    "instruction_text = f'Can you explain who are the Los Angeles Dodgers and what are they known for is in less than {max_output_tokens} tokens?'\n",
    "  \n",
    "data = {\n",
    "    'prompt': instruction_text,\n",
    "    'temperature': 0.5,\n",
    "    'top_p': 0.95,\n",
    "    'max_tokens': max_output_tokens,\n",
    "    'stream': streaming,\n",
    "    'model': model_selected\n",
    "    }\n",
    "\n",
    "# API Call\n",
    "llm_api(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a8b67a6",
   "metadata": {},
   "source": [
    "### II b. Python request Chat Completion\n",
    "\n",
    "This example demonstrates how to get responses from `mixtral-8x7b-instruct-v01`, `llamaguard-7b`, `mistral-7b-instruct-v03`, `phi-3-mini-128k-instruct`, `phi-3-5-moe-instruct`, `llama-3-8b-instruct`, `llama-3-1-8b-instruct`, `llama-3-2-3b-instruct`, `llama-3-3-70b-instruct` models using python request. We will be sending headers including API-Key and json data that includes the chat conversation content to `v1/chat/completions` endpoint, and the model generates a response. In this case, the conversation is about the Los Angeles Dodgers won the World Series in 2020. This is useful for building conversational AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "streaming = True\n",
    "max_output_tokens = 200\n",
    "\n",
    "# Available Models list\n",
    "available_models = [\"mixtral-8x7b-instruct-v01\", \"llamaguard-7b\", \"mistral-7b-instruct-v03\", \"phi-3-mini-128k-instruct\", \"phi-3-5-moe-instruct\", \"llama-3-8b-instruct\", \"llama-3-1-8b-instruct\", \"llama-3-2-3b-instruct\", \"llama-3-3-70b-instruct\"]\n",
    "\n",
    "# Let's select the model from available list\n",
    "model_selected = available_models[0]\n",
    "\n",
    "def stream_and_yield_response(response):\n",
    "    for chunk in response.iter_lines():\n",
    "        decoded_chunk = chunk.decode(\"utf-8\")\n",
    "        if decoded_chunk == \"data: [DONE]\":\n",
    "            pass\n",
    "        elif decoded_chunk.startswith(\"data: {\"):\n",
    "            payload = decoded_chunk.lstrip(\"data:\")\n",
    "            json_payload = json.loads(payload)\n",
    "\n",
    "            if ('role' in json_payload['choices'][0]['delta'] and json_payload['choices'][0]['delta']['role'] != None): \n",
    "                yield json_payload['choices'][0]['delta']['role'] + ': '\n",
    "            else:\n",
    "                yield json_payload['choices'][0]['delta']['content']\n",
    "\n",
    "# function from confluence\n",
    "def llm_api(data):\n",
    "    \"\"\"\n",
    "    Creates a request to Dev GenAI Text to Text model with API key in header.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://genai-api-dev.dell.com/v1/chat/completions\"\n",
    "\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'api-key': os.environ[\"DEV_GENAI_API_KEY\"],\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, stream=data['stream'], verify=certifi.where())\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if data['stream']:\n",
    "            for result in stream_and_yield_response(response):\n",
    "                print(result, end='')\n",
    "        else:\n",
    "            response_dict = response.json()\n",
    "            result = response_dict['choices'][0]['message']['role'] + ': ' + response_dict['choices'][0]['message']['content']\n",
    "            print(result)\n",
    "\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print('Error code:', err.response.status_code)\n",
    "        print('Error message:', err.response.text)\n",
    "    except Exception as err:\n",
    "        print('Error:', err)\n",
    "\n",
    "# Model instruction and Parameters\n",
    "messages =  [{'role': 'user', 'content': f'You are a helpful assistant who needs to anser in less than {max_output_tokens} tokens'},\n",
    "             {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'},\n",
    "             {'role': 'user', 'content': 'Who are the Los Angeles Dodgers?'}]\n",
    "\n",
    "data = {\n",
    "    'messages': messages,\n",
    "    'temperature': 0.5,\n",
    "    'top_p': 0.95,\n",
    "    'max_tokens': max_output_tokens,\n",
    "    'stream': streaming,\n",
    "    'model': model_selected\n",
    "    }\n",
    "\n",
    "# API Call\n",
    "llm_api(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
